---
about_this_resource_text: <h2 class="subhead">Summary</h2> <p>This video introduces
  students to the moments of a distribution and how they can be used to characterize
  the shape of a distribution. Students apply what they learned to examples of discrete
  and continuous distributions.</p> <h2 class="subhead">Learning Objectives</h2> <p>After
  watching this video students will be able to:</p> <ul> <li>Explain moments of distributions.</li>
  <li>Compute moments and understand what they mean.</li> </ul> <p>Funding provided
  by the Singapore University of Technology and Design (SUTD)</p> <p>Developed by
  the Teaching and Learning Laboratory (TLL) at MIT for SUTD</p> <p>MIT &copy; 2012</p>
course_id: res-tll-004-stem-concept-videos-fall-2013
embedded_media:
- id: Video-YouTube-MP4
  media_location: fv5QB3eK7jA
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Video-YouTube-MP4
  type: Video
  uid: 653168bcde9630e168294f7ac906d435
- id: Thumbnail-YouTube-JPG
  media_location: https://img.youtube.com/vi/fv5QB3eK7jA/default.jpg
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Thumbnail-YouTube-JPG
  type: Thumbnail
  uid: 6f43f7c73a4af5cb4686dc560a2ad6f7
- id: Video-iTunesU-MP4
  media_location: https://itunes.apple.com/us/podcast/moments-of-distributions/id765926614?i=317672235&mt=2
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Video-iTunes U-MP4
  type: Video
  uid: e11e14aeeb8a1d2150e69861b2af9e4d
- id: Video-InternetArchive-MP4
  media_location: https://archive.org/download/MITRES.TLL-004F13/MITRES_TLL-004F13_moments_of_distributions_300k.mp4
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Video-Internet Archive-MP4
  type: Video
  uid: d871c8d3f4c7604ec659da1cc47db187
- id: MITRES_TLL-004F13_MmntDist.pdf
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  technical_location: https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/probability-and-statistics/moments-of-distributions/MITRES_TLL-004F13_MmntDist.pdf
  title: Moments of Distributions Transcript
  type: null
  uid: 28afcaf798c470cbb0cc379d7682ff0f
- id: MITRES_TLL-004F13_Momnt_IG.pdf
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  technical_location: https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/probability-and-statistics/moments-of-distributions/MITRES_TLL-004F13_Momnt_IG.pdf
  title: Moments of Distributions Instructor Guide
  type: null
  uid: 379da633737b535582d36d2f731598fa
- id: 3Play-3PlayYouTubeid-MP4
  media_location: fv5QB3eK7jA
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: 3Play-3Play YouTube id
  type: 3Play
  uid: f561c3dda8c95990443849214b974eca
- id: fv5QB3eK7jA.srt
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  technical_location: https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/probability-and-statistics/moments-of-distributions/fv5QB3eK7jA.srt
  title: 3play caption file
  type: null
  uid: d02f6456dd612a590c74a06fd33f8dc7
- id: fv5QB3eK7jA.pdf
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  technical_location: https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/probability-and-statistics/moments-of-distributions/fv5QB3eK7jA.pdf
  title: 3play pdf file
  type: null
  uid: ffc5287f6e338afbc3889cd798d7aee3
- id: Caption-3Play YouTube id-SRT
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Caption-3Play YouTube id-SRT-English - US
  type: Caption
  uid: 11c3b47d37bf98e968881cdf6ab1f784
- id: Transcript-3Play YouTube id-PDF
  parent_uid: 4c0c398a25627ff24c741a2046fc4671
  title: Transcript-3Play YouTube id-PDF-English - US
  type: Transcript
  uid: 62f8d295cbfb17be73608245ea3f4a40
inline_embed_id: 27094158momentsofdistributions91031335
layout: video
order_index: null
parent_uid: a0c7838af9cddcf2a8b07bc9d6fc46d0
related_resources_text: <p>Instructor Guide</p> <p><a href="./resolveuid/379da633737b535582d36d2f731598fa"
  target="_blank">Moments of Distributions Instructor Guide (PDF)</a></p>
short_url: moments-of-distributions
technical_location: https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/probability-and-statistics/moments-of-distributions
template_type: Tabbed
title: Moments of Distributions
transcript: "<p><span m='5500'> Why does going to the airport seem to require extra\
  \ time compared with coming back from</span> <span m='9600'>the airport even if\
  \ the traffic is the same in both directions? The answer must somehow</span> <span\
  \ m='15080'>depend on more than just the average travel time, which we\u2019re assuming\
  \ is the same and</span> <span m='19900'>often is. In fact, it depends on the distribution\
  \ of travel times. Probability distributions</span> <span m='27769'>are fully described\
  \ by listing or graphing every probability. For example, how likely</span> <span\
  \ m='32980'>is a journey to the airport to be between 10 and 20 minutes? How likely\
  \ is a 20\u201430</span> <span m='38350'>minute journey? A 30\u201440 minute journey?\
  \ And so on. We\u2019ll answer the airport question</span> <span m='43550'>at the\
  \ end of the video.</span> <span m='45590'>This video is part of the Probability\
  \ and Statistics video series. Many natural and</span> <span m='50379'>social phenomena\
  \ are probabilistic in nature. Engineers, scientists, and policymakers often</span>\
  \ <span m='55960'>use probability to model and predict system behavior.</span> <span\
  \ m='59390'>Hi, my name is Sanjoy Mahajan, and I\u2019m a professor of Applied Science\
  \ and Engineering</span> <span m='64720'>at Olin College. Before watching this video,\
  \ you should be proficient with integration</span> <span m='70420'>and have some\
  \ familiarity with probabilities.</span> <span m='74020'>After watching this video,\
  \ you will be able to:</span> <span m='76920'>Explain what moments of distributions\
  \ are, and</span> <span m='80140'>Compute moments and understand what they mean</span>\
  \ <span m='87658'>To illustrate what a probability distribution is, lets consider\
  \ rolling two fair dice. The</span> <span m='94670'>probability distribution of\
  \ their sum is this table. For example, the only way to get a</span> <span m='99549'>sum\
  \ of two is to roll a 1 on each die. And, there are 36 possible rolls for a pair\
  \ of</span> <span m='106329'>dice. So, getting a sum of two has a probability of\
  \ 1 over 36. The probability of rolling a</span> <span m='113880'>sum of 3 is 2\
  \ over 36. And so on and so forth. You can fill in a table like this yourself.</span>\
  \ <span m='121090'>But the whole distribution, even for something as simple as two\
  \ dice, is usually too much</span> <span m='126219'>information.</span> <span m='127860'>We\
  \ often want to characterize the shape of the distribution using only a few numbers.</span>\
  \ <span m='132790'>Of course, that throws away information, but throwing away information\
  \ is the only way</span> <span m='139700'>to fit the complexity of the world into\
  \ our brains.</span> <span m='143200'>The art comes in keeping the most important\
  \ information. Finding the moments of a distribution</span> <span m='149040'>can\
  \ help us reach our goal. Two moments that you are probably already familiar with\
  \ are</span> <span m='154959'>mean and variance. They are the two most important\
  \ moments of distributions.</span> <span m='159690'>Let\u2019s define these moments\
  \ more formally. The mean is the first moment of a distribution.</span> <span m='167150'>It\
  \ is also called the expected value and is computed as shown. Expected value of\
  \ x, that\u2019s</span> <span m='174599'>x with angled brackets around it, is equal\
  \ to this sum. It\u2019s the weighted sum of all</span> <span m='180349'>of the\
  \ x\u2019s weighted by their probabilities. Let the x sub i be the possible values\
  \ of</span> <span m='186069'>x.</span> <span m='187569'>For example, for the rolling\
  \ of two dice, the possible values for x sub i would be 2,3,4</span> <span m='194400'>all\
  \ the way up through 12. And p sub i would be the corresponding probabilities of\
  \ rolling</span> <span m='199220'>those sums - so that was 1 over 36, 2 over 36,\
  \ and so on.</span> <span m='204540'>So, the first moment gives us some idea of\
  \ what our distribution might look like, but</span> <span m='209840'>not much. Think\
  \ about it like this, the center of mass in these two images is in the same</span>\
  \ <span m='214900'>place, but the mass is actually distributed very differently\
  \ in the two cases. We need</span> <span m='219930'>more information.</span> <span\
  \ m='221409'>The second moment can help us. The second moment is very similar in\
  \ structure to the</span> <span m='226099'>first moment. We write it the same way\
  \ with angled brackets, but now we\u2019re talking about</span> <span m='231819'>the\
  \ expected value of x squared. So it\u2019s still a sum and it\u2019s still weighted\
  \ by the</span> <span m='238379'>probabilities p sub i, but now we square each possible\
  \ x value. For the dice example that</span> <span m='244340'>was the values from\
  \ two through twelve. This is also called the mean square. First you</span> <span\
  \ m='250920'>square the x values, then you take the mean, weighting each x sub i\
  \ by its probability,</span> <span m='256829'>p sub i.</span> <span m='257779'>In\
  \ general, the nth moment is defined as follows.</span> <span m='267590'>So how\
  \ does the second moment help us get a better picture of our distribution? Because</span>\
  \ <span m='272479'>it can help us calculate something called the variance. The variance\
  \ measures how spread</span> <span m='278300'>out the distribution is around the\
  \ mean. To calculate the variance, you first subtract</span> <span m='284229'>the\
  \ mean from each x sub i \u2013 this is like finding the distance of each x sub\
  \ i from</span> <span m='289710'>the mean - and then you square the result and multiply\
  \ by p sub i.</span> <span m='299620'>What are the dimensions of the variance? The\
  \ square of the dimensions of x. For example</span> <span m='304930'>if the dimension\
  \ is a length, then the variance is a length squared. But we often want a measure</span>\
  \ <span m='310660'>of dispersion like the variance, but one that has the same dimensions\
  \ as x itself. That</span> <span m='316490'>measure is the standard deviation, sigma.\
  \ Sigma is defined as the square root of the</span> <span m='322320'>variance. So\
  \ if the variable x has dimensions of length, then the variance will have dimensions</span>\
  \ <span m='327520'>of length squared, but the standard deviation, sigma, will have\
  \ dimensions of length so it\u2019s</span> <span m='332470'>comparable to x directly.</span>\
  \ <span m='335000'>This expression for the variance looks like a pain to compute,\
  \ but it has an alternative</span> <span m='340350'>expression that is much simpler.\
  \ And you get to show that as one of the exercises after</span> <span m='345320'>the\
  \ video. The alternative expression, the much simpler one, is that the variance\
  \ is</span> <span m='351490'>equal to the second moment, our old friend, minus the\
  \ square of the first moment, or the</span> <span m='357159'>mean.</span> <span\
  \ m='358240'>Pause the video here to convince yourself that this difference is always\
  \ non-negative.</span> <span m='369729'>This alternative expression for the variance,\
  \ this much more useful one, is also the parallel</span> <span m='375050'>axis theorem\
  \ in mechanics, which says that the moment of inertia of an object about the</span>\
  \ <span m='379990'>center of mass is equal to the moment of inertia about an axis\
  \ shifted by h from the center</span> <span m='385160'>of mass, a parallel shift,\
  \ minus mh squared.</span> <span m='389860'>So how does this analogy work? This,\
  \ the dispersion around the mean, which is here at the center</span> <span m='396350'>of\
  \ mass, is like the variance. This is like the second moment if we make h equal\
  \ to the</span> <span m='402610'>mean. So this is the dispersion around zero or\
  \ its second moment. So this is like x squared,</span> <span m='410389'>the expected\
  \ value. The mass is the sum total of all the weights here for each of xi which</span>\
  \ <span m='416580'>all add up to one. So this is just like one in this problem.\
  \ And then the h squared, well</span> <span m='423639'>h is the mean, so this is\
  \ x squared.</span> <span m='426840'>So you can see the exact same structure repeated\
  \ with h, the shift of axis as the mean, and</span> <span m='432910'>m the mass,\
  \ as the sum of all probabilities which is one. So this formula for the variance</span>\
  \ <span m='439500'>is also the parallel axis theorem.</span> <span m='446900'>Let\u2019\
  s use the definitions of the moments, and also of the related quantity, the variance,</span>\
  \ <span m='452110'>and practice on a few distributions.</span> <span m='454460'>A\
  \ simple discrete distribution is a single coin flip. Instead of thinking of the\
  \ coin</span> <span m='459639'>flip as resulting in heads or tails, let\u2019s think\
  \ about the coin as turning up a zero</span> <span m='463889'>or one. Let p be the\
  \ probability of a one.</span> <span m='467970'>So the mean is the weighted sum\
  \ of the xi\u2019s, weighted by the probabilities. So the mean</span> <span m='473560'>x\
  \ is the sum pi xi which is equal to one minus p times zero plus p times one which\
  \ is equal</span> <span m='482349'>to p.</span> <span m='483620'>What about the\
  \ second moment? X squared, it\u2019s equal to the weighted sum of the xi\u2019\
  s squared</span> <span m='491050'>so the weights are the same and we can square\
  \ each value here, the xi\u2019s, but since they\u2019re</span> <span m='498970'>all\
  \ zero or one, squaring doesn\u2019t change them. So the second moment and the third\
  \ moment</span> <span m='504919'>and every higher moment are all p. Pause the video\
  \ here and compute the variance and sketch</span> <span m='512760'>it as a function\
  \ of p.</span> <span m='520690'>The variance from our old convenient form of the\
  \ formula is\u2026 variance of x is the</span> <span m='524750'>mean squared, mean\
  \ square minus the squared mean and all the moments themselves were just</span>\
  \ <span m='529680'>p. So that\u2019s p minus p squared which is equal to p times\
  \ 1 minus p.</span> <span m='536580'>What does that look like? We sketch it. P on\
  \ this axis, variance on that axis and the</span> <span m='543339'>curve starts\
  \ at zero (something I can\u2019t understand) and goes back to zero.</span> <span\
  \ m='549310'>This is a p equals 1 and that\u2019s p equals zero. Does that make\
  \ sense?</span> <span m='555450'>Yeah, it does\u2026 from the meaning of variance\
  \ as dispersion around the mean. So take the</span> <span m='562080'>first extreme\
  \ case of p equals zero. In other words, the coin has no chance of producing</span>\
  \ <span m='567430'>a one, always produces a zero every time. There the mean is zero\
  \ and there is no dispersion</span> <span m='573730'>because it always produces\
  \ zero. The same applies when p equals one here at this extreme.</span> <span m='580060'>The\
  \ coin always produces a one with no dispersion. There is no variation, there is\
  \ no variance</span> <span m='585560'>and it\u2019s plausible that the variance\
  \ should be a maximum right in between\u2026 here at p</span> <span m='592420'>equals\
  \ one half which it is on this curve. So everything looks good. Our calculation</span>\
  \ <span m='599100'>seems reasonable and checks out in the extreme cases.</span>\
  \ <span m='603540'>Before we go back to the airport problem, let\u2019s extend the\
  \ idea of moments to continuous</span> <span m='607900'>distributions.</span> <span\
  \ m='609630'>Here, instead of a list of probabilities for each possible x, we have\
  \ a probability density</span> <span m='614459'>p as a function of x, where x is\
  \ now a continuous variable. That\u2019s the continuous version</span> <span m='621010'>for\
  \ the nth moment was a sum of xi to the nth weighted by the probabilities. Here,\
  \ the</span> <span m='627880'>nth moment, x sub n, in equal to instead of a sum,\
  \ an integral. Weighted again, as always,</span> <span m='634540'>by the probability\
  \ times x sub n, as before and with a dx because p of x times dx is the</span> <span\
  \ m='642340'>probability and you add them all up over all possible values of x.\
  \ That\u2019s the formula</span> <span m='648399'>for a continuous distribution,\
  \ for the moments of a continuous distribution.</span> <span m='650769'>Let\u2019\
  s practice on the simplest continuous distribution, the uniform distribution. X</span>\
  \ <span m='657170'>is equally likely to be any real number between zero and one.\
  \ That\u2019s the distribution and</span> <span m='663420'>we can compute the first\
  \ and second moments and the variance.</span> <span m='667450'>Pause the video here,\
  \ use the definition of moments for a continuous distribution and</span> <span m='672720'>compute\
  \ the mean, first moment, the second moment, and from those two, the variance.</span>\
  \ <span m='687930'>What you should have found is \u2026 for the mean, it\u2019s\
  \ the integral of one because p</span> <span m='693240'>of x is one, times x between\
  \ zero and one dx, which is x squared over two evaluated</span> <span m='700990'>between\
  \ zero and one, which equal one half\u2026 which makes sense. The mean here, the\
  \ average</span> <span m='707649'>value is just one-half right in the middle of\
  \ the distribution of the possible values</span> <span m='712680'>of x.</span> <span\
  \ m='714279'>What about the mean square? For that, you should have found almost\
  \ the same calculation, one</span> <span m='722160'>times x squared dx, which equals\
  \ x cubed over 3 between zero and one equals one-third. And</span> <span m='729519'>thus,\
  \ the variance is equal to one-third, that\u2019s the mean square minus the squared</span>\
  \ <span m='735200'>mean, which is\u2026 one twelfth. And that number is familiar.\
  \ That\u2019s the same 1/12 that shows</span> <span m='742730'>up in the moment\
  \ of inertia of a ruler of length l and mass m. Its moment of inertia</span> <span\
  \ m='748500'>is 1/12 ml squared which illustrates again the connection between moments\
  \ of inertia</span> <span m='754600'>and moments of distributions.</span> <span\
  \ m='756570'>Let\u2019s apply our knowledge to understand quantitatively, or in\
  \ a formal way, what happens</span> <span m='762589'>with airport travel \u2013\
  \ why does it seem so much longer on the way there, than on the</span> <span m='767600'>way\
  \ back?</span> <span m='768370'>Here is the ideal travel experience to the airport,\
  \ the distribution of travel times</span> <span m='773839'>t. Here's the probability\
  \ of each particular travel time, p</span> <span m='779079'>of t. In the ideal world,\
  \ the travel time would be very predictable. Let\u2019s say it</span> <span m='786230'>would\
  \ be almost always twenty minutes. In that case, you would allow twenty minutes</span>\
  \ <span m='791570'>to get to the airport and you would allow twenty minutes on the\
  \ way back. Going there</span> <span m='796329'>and coming back would seem the same.</span>\
  \ <span m='798399'>But, here\u2019s what travel to the airport actually looks like.\
  \ Let\u2019s say the mean is still</span> <span m='804070'>the same, but the reality\
  \ is that there\u2019s lots of dispersion. And so the curve actually</span> <span\
  \ m='810139'>looks like that. Sometimes the travel time will be 30 minutes, sometimes\
  \ 40, sometimes</span> <span m='816360'>10.</span> <span m='818630'>So now, what\
  \ do you have to do?... this is reality. Well, on the way home, it\u2019s no</span>\
  \ <span m='824079'>problem. On average, you get home in twenty minutes. You leave\
  \ whenever you get out of</span> <span m='828680'>the baggage claim. And while it\u2019\
  s true that the trip to the airport follows the same distribution,</span> <span\
  \ m='834269'>the risk to you of not making it to the airport on time is much greater.\
  \ If you just allow</span> <span m='839639'>twenty minutes, yeah, sometimes you\u2019\
  ll get lucky, but every once in a while it will take</span> <span m='843800'>you\
  \ twenty-five or thirty minutes.</span> <span m='846410'>So what you have to do\
  \ is allow more time on the way there so that you don\u2019t miss</span> <span m='849740'>your\
  \ flight - maybe thirty minutes, maybe even forty minutes. It all depends on the</span>\
  \ <span m='854540'>dispersion, or standard deviation, of the distribution. On the\
  \ way to the airport, you</span> <span m='859730'>are much more aware of the distribution,\
  \ if you will, than you are on the way back.</span> <span m='869400'>In this video,\
  \ we saw how to calculate the moments of a distribution and how these moments can</span>\
  \ <span m='874320'>help us quickly summarize the distribution. Like life...</span>\
  \ <span m='879000'>when something is complicated, simplify it, grasp it, and understand\
  \ it by appreciating its moments!</span> </p>"
type: course
uid: 4c0c398a25627ff24c741a2046fc4671

---
None